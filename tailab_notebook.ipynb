{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NibM0SsCn76"
   },
   "source": [
    "<h1>TAILab Solution for Urinary Stone Challenge</h1>\n",
    "<h4 style='text-align: right;'> Made by TAIL </h4>\n",
    "\n",
    "----\n",
    "<h3>Introduction</h3>\n",
    "This jupyter notebook provide full code which we used to train deep learning model in MOAI MCRC Deep Learning Challenge for urinary stone challenge. During the challenge we have to segment urinary stones from the given CT images.\n",
    "<h3>Tools</h3>\n",
    "<ul>\n",
    " <li>jupyter notebook</li>\n",
    " <li>pytorch(1.1.0)</li>\n",
    " <li>imgaug (0.4.0)</li>\n",
    " <li>cv2 (4.1)</li>\n",
    " <li>SimpleITK(1.2.0)</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Data</h3>\n",
    "Computed tomography(CT) Slices of urinary stone are provided in the follow configuration.\n",
    "\n",
    "![inbox_5086226_75f678c62158e2e2271c34b2cc9f5af6_1.png](data:image/png;base64,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)\n",
    "\n",
    "<ul>\n",
    "<li>Train: 600 slices</li>\n",
    "<li>Valid: 100 slices</li>\n",
    "<li>Test: 200 slices</li>\n",
    "</ul>\n",
    "\n",
    "Datasets are divided into Train, Validation and Test. As shown in the figure above, CT files are in DICOM format and LABEL file in PNG. DICOM filename and PNG filename are configured to match. Each DICOM file is an anonymous CT file, and LABEL file is expressed only the masked part in red.\n",
    "\n",
    "<h3>How we trained our model</h3>\n",
    "We train Modified-Unet from the scratch for 150 epochs with a batch size of 16. We use Adam optimizer and Dice loss as the loss function. Learning rate is scheduled by Epoch-based decay and the initial learning rate set to 2e-5. Mean Intersection over Union(mIoU) is used for evaluation.\n",
    "\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRimKkA2E4-_"
   },
   "source": [
    "<h2>Table of contents</h2>\n",
    "\n",
    "> <h4>Basic Settings</h4>\n",
    "\n",
    "- [Hyper-parameters](#Hyper-parameters)\n",
    "- [Import Libraries](#Libraries)\n",
    "\n",
    "> <h4>Data explore</h4>\n",
    "\n",
    "- [plot functions](#plot)\n",
    "- [Visualize sample data](#visualized)\n",
    "\n",
    "> <h4>Model</h4>\n",
    "\n",
    "- [Network](#Network)\n",
    "- [Data Pre-processing](#Pre-processing)\n",
    "- [Dataset and Dataloader](#Dataloader)\n",
    "- [Optimizer and Loss](#Optimizer)\n",
    "- [ETC](#ETC)\n",
    "- [Evaluation Metrics](#EvaluationMetrics)\n",
    "- [Learning rate](#LR)\n",
    "- [Core functions](#corefunctions)\n",
    "\n",
    "> <h4>Main code</h4>\n",
    "\n",
    " - [Train](#train)\n",
    " - [Evaluation](#evaluation)\n",
    " - [Inference](#Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-oIAKIaPPnH"
   },
   "source": [
    "# Basic Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq51K7XUCn77"
   },
   "source": [
    "## Hyper-parameters\n",
    "<a id=\"Hyper-parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GoXtIHXCn78"
   },
   "outputs": [],
   "source": [
    "# Directory\n",
    "data_root = '../input/urinary-stone-challenge/DataSet'\n",
    "save_dir = './result'\n",
    "\n",
    "# Input image\n",
    "crop_size = 300\n",
    "input_size = 224\n",
    "w_min = -100.\n",
    "w_max = 300.\n",
    "\n",
    "# Data augmentation\n",
    "rot_factor = 30. \n",
    "scale_factor = 0.15\n",
    "flip = True\n",
    "trans_factor = 0.1\n",
    "\n",
    "# Network\n",
    "base_n_filter = 32\n",
    "\n",
    "# Optimizer\n",
    "lr = 2e-5\n",
    "lr_decay_epoch = '130,145'\n",
    "lr_warmup_epoch = 0\n",
    "no_bias_decay = True\n",
    "\n",
    "# Hyper-parameter\n",
    "batch_size = 16\n",
    "start_epoch = 0\n",
    "max_epoch = 150\n",
    "conf_threshold = 0.9\n",
    "\n",
    "# Loss function\n",
    "loss = 'dice'\n",
    "\n",
    "# Resume trained network\n",
    "resume = ''\n",
    "\n",
    "# Resource option\n",
    "workers = 10\n",
    "gpu_id = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oc5MZoNoCn8A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make output directory\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# GPU Setting\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu_id\n",
    "ngpu = len(gpu_id.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGdrGa5SCn8D"
   },
   "source": [
    "## Import Libraries\n",
    "<a id=\"Libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFJOlceECn8E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import uniform\n",
    "from imgaug import augmenters as iaa\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from glob import glob\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCF382SbCn8I"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.fastest = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2s1Xa0TCn8L"
   },
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovqWWwpoVVnt"
   },
   "source": [
    "# Data explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrUTek8nVWDE"
   },
   "source": [
    "## Plot functions\n",
    "<a id=\"plot\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sM646grJVL0w"
   },
   "outputs": [],
   "source": [
    "def overlay_mask_on_img(img, mask):\n",
    "    overlay_img = (img - img.min()) / (img.max() - img.min())\n",
    "    overlay_img = (overlay_img * 255).astype(np.uint8)\n",
    "    overlay_img = np.repeat(overlay_img[...,None], 3, -1)\n",
    "    overlay_img[mask != 0] = [255,0,0]\n",
    "    \n",
    "    return overlay_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgbMKXiXTVj9"
   },
   "outputs": [],
   "source": [
    "def plot_images(ori,mask,overl)\n",
    "  fig, axes = plt.subplots(1, 3, figsize=(15,7))\n",
    "\n",
    "  axes[0].imshow(ori, 'gray')\n",
    "  axes[0].set_title(\"Original Image\", fontsize=15)\n",
    "\n",
    "  axes[1].imshow(mask, 'gray')\n",
    "  axes[1].set_title(\"Original Mask\", fontsize=15)\n",
    "\n",
    "  axes[2].imshow(overl)\n",
    "  axes[2].set_title(\"Image with Mask\", fontsize=15)\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eeUbvJIVfn5"
   },
   "source": [
    "## Visualize sample data\n",
    "<a id=\"visualized\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fDCQaFBVLxh"
   },
   "outputs": [],
   "source": [
    "# load sample data\n",
    "sample_img_1  = '../DataSet/TRAIN/DCM/123.dcm'\n",
    "sample_mask_1 = '../DataSet/TRAIN/Label/123.png'\n",
    "\n",
    "sample_img_2  = '../DataSet/Valid/DCM/626.dcm'\n",
    "sample_mask_2 = '../DataSet/Valid/Label/626.png'\n",
    "\n",
    "# img2numpy\n",
    "img_train = sitk.ReadImage(sample_img_1)\n",
    "img_arr_train = sitk.GetArrayFromImage(img_train)[0]\n",
    "mask_train = cv2.imread(sample_mask_1, 0)\n",
    "\n",
    "img_valid = sitk.ReadImage(sample_img_2)\n",
    "img_arr_valid = sitk.GetArrayFromImage(img_valid)[0]\n",
    "mask_valid = cv2.imread(sample_mask_2, 0)\n",
    "\n",
    "# overay images\n",
    "overlay_img_train = overlay_mask_on_img(img_arr_train, mask_train)\n",
    "overlay_img_valid = overlay_mask_on_img(img_arr_valid, mask_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ryAHCjHT0fE"
   },
   "outputs": [],
   "source": [
    "plot_images(img_arr_train,mask_train,overlay_img_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdT9xZE3T-bK"
   },
   "outputs": [],
   "source": [
    "plot_images(img_arr_valid,mask_valid,overlay_img_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgHjwTChPIrm"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inPXdhWjCn8P"
   },
   "source": [
    "## Network\n",
    "<a id=\"Network\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZD8reT2oCn8P"
   },
   "outputs": [],
   "source": [
    "class Modified2DUNet(nn.Module):\n",
    "    '''\n",
    "    Reference : https://github.com/pykao/Modified-3D-UNet-Pytorch\n",
    "    Description : We changed original 3D version of Modified U-Net to 2D version.\n",
    "    '''\n",
    "    def __init__(self, in_channels, n_classes, base_n_filter = 8):\n",
    "        super(Modified2DUNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.base_n_filter = base_n_filter\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.dropout3d = nn.Dropout3d(p=0.6)\n",
    "        \n",
    "\n",
    "        # Level 1 context pathway\n",
    "        self.conv3d_c1_1 = nn.Conv2d(self.in_channels, self.base_n_filter, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv3d_c1_2 = nn.Conv2d(self.base_n_filter, self.base_n_filter, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.lrelu_conv_c1 = self.lrelu_conv(self.base_n_filter, self.base_n_filter)\n",
    "        self.gnorm3d_c1 = nn.GroupNorm(self.base_n_filter//2, self.base_n_filter)\n",
    "\n",
    "        # Level 2 context pathway\n",
    "        self.conv3d_c2 = nn.Conv2d(self.base_n_filter, self.base_n_filter*2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.norm_lrelu_conv_c2 = self.norm_lrelu_conv(self.base_n_filter*2, self.base_n_filter*2)\n",
    "        self.norm_lrelu_conv_c2 = self.norm_lrelu_conv(self.base_n_filter*2, self.base_n_filter*2)\n",
    "        self.norm_lrelu_conv_c2 = self.norm_lrelu_conv(self.base_n_filter*2, self.base_n_filter*2)\n",
    "        self.gnorm3d_c2 = nn.GroupNorm(self.base_n_filter, self.base_n_filter*2)\n",
    "\n",
    "        # Level 3 context pathway\n",
    "        self.conv3d_c3 = nn.Conv2d(self.base_n_filter*2, self.base_n_filter*4, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.norm_lrelu_conv_c3 = self.norm_lrelu_conv(self.base_n_filter*4, self.base_n_filter*4)\n",
    "        self.norm_lrelu_conv_c3 = self.norm_lrelu_conv(self.base_n_filter*4, self.base_n_filter*4)\n",
    "        self.norm_lrelu_conv_c3 = self.norm_lrelu_conv(self.base_n_filter*4, self.base_n_filter*4)\n",
    "        self.gnorm3d_c3 = nn.GroupNorm(self.base_n_filter, self.base_n_filter*4)\n",
    "\n",
    "        # Level 4 context pathway\n",
    "        self.conv3d_c4 = nn.Conv2d(self.base_n_filter*4, self.base_n_filter*8, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.norm_lrelu_conv_c4 = self.norm_lrelu_conv(self.base_n_filter*8, self.base_n_filter*8)\n",
    "        self.norm_lrelu_conv_c4 = self.norm_lrelu_conv(self.base_n_filter*8, self.base_n_filter*8)\n",
    "        self.norm_lrelu_conv_c4 = self.norm_lrelu_conv(self.base_n_filter*8, self.base_n_filter*8)\n",
    "        self.gnorm3d_c4 = nn.GroupNorm(self.base_n_filter*2, self.base_n_filter*8)\n",
    "\n",
    "        # Level 5 context pathway, level 0 localization pathway\n",
    "        self.conv3d_c5 = nn.Conv2d(self.base_n_filter*8, self.base_n_filter*16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.norm_lrelu_conv_c5 = self.norm_lrelu_conv(self.base_n_filter*16, self.base_n_filter*16)\n",
    "        self.norm_lrelu_conv_c5 = self.norm_lrelu_conv(self.base_n_filter*16, self.base_n_filter*16)\n",
    "        self.norm_lrelu_conv_c5 = self.norm_lrelu_conv(self.base_n_filter*16, self.base_n_filter*16)\n",
    "        self.norm_lrelu_upscale_conv_norm_lrelu_l0_1 = self.norm_lrelu_upscale_conv_norm_lrelu_1(self.base_n_filter*16)\n",
    "        self.norm_lrelu_upscale_conv_norm_lrelu_l0_2 = self.norm_lrelu_upscale_conv_norm_lrelu_2(self.base_n_filter*16, self.base_n_filter*8)\n",
    "\n",
    "        self.conv3d_l0 = nn.Conv2d(self.base_n_filter*8, self.base_n_filter*8, kernel_size = 1, stride=1, padding=0, bias=False)\n",
    "        self.gnorm3d_l0 = nn.GroupNorm(self.base_n_filter*2, self.base_n_filter*8)\n",
    "\n",
    "        # Level 1 localization pathway\n",
    "        self.conv_norm_lrelu_l1 = self.conv_norm_lrelu(self.base_n_filter*16, self.base_n_filter*16)\n",
    "        self.conv3d_l1 = nn.Conv2d(self.base_n_filter*16, self.base_n_filter*8, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.norm_lrelu_upscale_conv_norm_lrelu_l1_1 = self.norm_lrelu_upscale_conv_norm_lrelu_1(self.base_n_filter*8)\n",
    "        self.norm_lrelu_upscale_conv_norm_lrelu_l1_2 = self.norm_lrelu_upscale_conv_norm_lrelu_2(self.base_n_filter*8, self.base_n_filter*4)\n",
    "\n",
    "        # Level 2 localization pathway\n",
    "        self.conv_norm_lrelu_l2 = self.conv_norm_lrelu(self.base_n_filter*8, self.base_n_filter*8)\n",
    "        self.conv3d_l2 = nn.Conv2d(self.base_n_filter*8, self.base_n_filter*4, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.norm_lrelu_upscale_conv_norm_lrelu_l2_1 = self.norm_lrelu_upscale_conv_norm_lrelu_1(self.base_n_filter*4)\n",
    "        self.norm_lrelu_upscale_conv_norm_lrelu_l2_2 = self.norm_lrelu_upscale_conv_norm_lrelu_2(self.base_n_filter*4, self.base_n_filter*2)\n",
    "\n",
    "        # Level 3 localization pathway\n",
    "        self.conv_norm_lrelu_l3 = self.conv_norm_lrelu(self.base_n_filter*4, self.base_n_filter*4)\n",
    "        self.conv3d_l3 = nn.Conv2d(self.base_n_filter*4, self.base_n_filter*2, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.norm_lrelu_upscale_conv_norm_lrelu_l3_1 = self.norm_lrelu_upscale_conv_norm_lrelu_1(self.base_n_filter*2)\n",
    "        self.norm_lrelu_upscale_conv_norm_lrelu_l3_2 = self.norm_lrelu_upscale_conv_norm_lrelu_2(self.base_n_filter*2, self.base_n_filter)\n",
    "\n",
    "        # Level 4 localization pathway\n",
    "        self.conv_norm_lrelu_l4 = self.conv_norm_lrelu(self.base_n_filter*2, self.base_n_filter*2)\n",
    "        self.conv3d_l4 = nn.Conv2d(self.base_n_filter*2, self.n_classes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        self.ds2_1x1_conv3d = nn.Conv2d(self.base_n_filter*8, self.n_classes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.ds3_1x1_conv3d = nn.Conv2d(self.base_n_filter*4, self.n_classes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        \n",
    "    def conv_norm_lrelu(self, feat_in, feat_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(feat_in, feat_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.GroupNorm(feat_out//2, feat_out),\n",
    "            nn.LeakyReLU())\n",
    "\n",
    "    def norm_lrelu_conv(self, feat_in, feat_out):\n",
    "        return nn.Sequential(\n",
    "            nn.GroupNorm(feat_in//2, feat_in),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(feat_in, feat_out, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "\n",
    "    def lrelu_conv(self, feat_in, feat_out):\n",
    "        return nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(feat_in, feat_out, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "\n",
    "    def norm_lrelu_upscale_conv_norm_lrelu_1(self, feat_in):\n",
    "        return nn.Sequential(\n",
    "            nn.GroupNorm(feat_in//2, feat_in),\n",
    "            nn.LeakyReLU())\n",
    "\n",
    "    def norm_lrelu_upscale_conv_norm_lrelu_2(self, feat_in, feat_out):\n",
    "        return nn.Sequential(\n",
    "            # should be feat_in*2 or feat_in\n",
    "            nn.Conv2d(feat_in, feat_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.GroupNorm(feat_out//2, feat_out),\n",
    "            nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  Level 1 context pathway\n",
    "        out = self.conv3d_c1_1(x)\n",
    "        residual_1 = out\n",
    "        out = self.lrelu(out)\n",
    "        out = self.conv3d_c1_2(out)\n",
    "        out = self.dropout3d(out)\n",
    "        out = self.lrelu_conv_c1(out)\n",
    "        # Element Wise Summation\n",
    "        out += residual_1\n",
    "        context_1 = self.lrelu(out)\n",
    "        out = self.gnorm3d_c1(out)\n",
    "        out = self.lrelu(out)\n",
    "\n",
    "        # Level 2 context pathway\n",
    "        out = self.conv3d_c2(out)\n",
    "        residual_2 = out\n",
    "        out = self.norm_lrelu_conv_c2(out)\n",
    "        out = self.norm_lrelu_conv_c2(out)\n",
    "        out = self.dropout3d(out)\n",
    "        out = self.norm_lrelu_conv_c2(out)\n",
    "        out = self.norm_lrelu_conv_c2(out)\n",
    "        out += residual_2\n",
    "        out = self.gnorm3d_c2(out)\n",
    "        out = self.lrelu(out)\n",
    "        context_2 = out\n",
    "\n",
    "        # Level 3 context pathway\n",
    "        out = self.conv3d_c3(out)\n",
    "        residual_3 = out\n",
    "        out = self.norm_lrelu_conv_c3(out)\n",
    "        out = self.norm_lrelu_conv_c3(out)\n",
    "        out = self.dropout3d(out)\n",
    "        out = self.norm_lrelu_conv_c3(out)\n",
    "        out = self.norm_lrelu_conv_c3(out)\n",
    "        out += residual_3\n",
    "        out = self.gnorm3d_c3(out)\n",
    "        out = self.lrelu(out)\n",
    "        context_3 = out\n",
    "\n",
    "        # Level 4 context pathway\n",
    "        out = self.conv3d_c4(out)\n",
    "        residual_4 = out\n",
    "        out = self.norm_lrelu_conv_c4(out)\n",
    "        out = self.norm_lrelu_conv_c4(out)\n",
    "        out = self.dropout3d(out)\n",
    "        out = self.norm_lrelu_conv_c4(out)\n",
    "        out = self.norm_lrelu_conv_c4(out)\n",
    "        out += residual_4\n",
    "        out = self.gnorm3d_c4(out)\n",
    "        out = self.lrelu(out)\n",
    "        context_4 = out\n",
    "\n",
    "        # Level 5\n",
    "        out = self.conv3d_c5(out)\n",
    "        residual_5 = out\n",
    "        out = self.norm_lrelu_conv_c5(out)\n",
    "        out = self.norm_lrelu_conv_c5(out)\n",
    "        out = self.dropout3d(out)\n",
    "        out = self.norm_lrelu_conv_c5(out)\n",
    "        out = self.norm_lrelu_conv_c5(out)\n",
    "        out += residual_5\n",
    "        out = self.norm_lrelu_upscale_conv_norm_lrelu_l0_1(out)\n",
    "        out = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.norm_lrelu_upscale_conv_norm_lrelu_l0_2(out)\n",
    "\n",
    "        out = self.conv3d_l0(out)\n",
    "        out = self.gnorm3d_l0(out)\n",
    "        out = self.lrelu(out)\n",
    "\n",
    "        # Level 1 localization pathway\n",
    "        out = F.interpolate(out, size = context_4.size()[-2:])\n",
    "        out = torch.cat([out, context_4], dim=1)\n",
    "        out = self.conv_norm_lrelu_l1(out)\n",
    "        out = self.conv3d_l1(out)\n",
    "        out = self.norm_lrelu_upscale_conv_norm_lrelu_l1_1(out)\n",
    "        out = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.norm_lrelu_upscale_conv_norm_lrelu_l1_2(out)\n",
    "\n",
    "\n",
    "        # Level 2 localization pathway\n",
    "        out = F.interpolate(out, size = context_3.size()[-2:])\n",
    "        out = torch.cat([out, context_3], dim=1)\n",
    "        out = self.conv_norm_lrelu_l2(out)\n",
    "        ds2 = out\n",
    "        out = self.conv3d_l2(out)\n",
    "        out = self.norm_lrelu_upscale_conv_norm_lrelu_l2_1(out)\n",
    "        out = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.norm_lrelu_upscale_conv_norm_lrelu_l2_2(out)\n",
    "\n",
    "        # Level 3 localization pathway\n",
    "        out = F.interpolate(out, size = context_2.size()[-2:])\n",
    "        out = torch.cat([out, context_2], dim=1)\n",
    "        out = self.conv_norm_lrelu_l3(out)\n",
    "        ds3 = out\n",
    "        out = self.conv3d_l3(out)\n",
    "        out = self.norm_lrelu_upscale_conv_norm_lrelu_l3_1(out)\n",
    "        out = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.norm_lrelu_upscale_conv_norm_lrelu_l3_2(out)\n",
    "\n",
    "        # Level 4 localization pathway\n",
    "        out = F.interpolate(out, size = context_1.size()[-2:])\n",
    "        out = torch.cat([out, context_1], dim=1)\n",
    "        out = self.conv_norm_lrelu_l4(out)\n",
    "        out_pred = self.conv3d_l4(out)\n",
    "\n",
    "        ds2_1x1_conv = self.ds2_1x1_conv3d(ds2)\n",
    "        ds1_ds2_sum_upscale = F.interpolate(ds2_1x1_conv, scale_factor=2, mode='nearest')\n",
    "        ds3_1x1_conv = self.ds3_1x1_conv3d(ds3)\n",
    "        ds1_ds2_sum_upscale = F.interpolate(ds1_ds2_sum_upscale, size = ds3_1x1_conv.size()[-2:])\n",
    "        ds1_ds2_sum_upscale_ds3_sum = ds1_ds2_sum_upscale + ds3_1x1_conv\n",
    "        ds1_ds2_sum_upscale_ds3_sum_upscale = F.interpolate(ds1_ds2_sum_upscale_ds3_sum, scale_factor=2, mode='nearest')\n",
    "\n",
    "        out = out_pred + ds1_ds2_sum_upscale_ds3_sum_upscale\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-GDVnheCn8U"
   },
   "outputs": [],
   "source": [
    "net = Modified2DUNet(in_channels=1, n_classes=1, base_n_filter=base_n_filter)\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lVDtN9YCn8Y"
   },
   "source": [
    "## Data Pre-processing\n",
    "<a id=\"Pre-processing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiuR2S82Cn8Z"
   },
   "outputs": [],
   "source": [
    "def image_windowing(img, w_min=0, w_max=300):\n",
    "    img_w = img.copy()\n",
    "\n",
    "    img_w[img_w < w_min] = w_min\n",
    "    img_w[img_w > w_max] = w_max\n",
    "\n",
    "    return img_w\n",
    "    \n",
    "def image_minmax(img):\n",
    "    img_minmax = ((img - np.min(img)) / (np.max(img) - np.min(img))).copy()\n",
    "    img_minmax = (img_minmax * 255).astype(np.uint8)\n",
    "        \n",
    "    return img_minmax\n",
    "\n",
    "def mask_binarization(mask, threshold=None):\n",
    "    if threshold is None:\n",
    "        threshold = 0.5\n",
    "\n",
    "    if isinstance(mask, np.ndarray):\n",
    "        mask_binarized = (mask > threshold).astype(np.uint8)\n",
    "    \n",
    "    elif isinstance(mask, torch.Tensor):\n",
    "        zeros = torch.zeros_like(mask)\n",
    "        ones = torch.ones_like(mask)\n",
    "        \n",
    "        mask_binarized = torch.where(mask > threshold, ones, zeros)\n",
    "    \n",
    "    return mask_binarized\n",
    "\n",
    "def augment_imgs_and_masks(imgs, masks, rot_factor, scale_factor, trans_factor, flip):\n",
    "    rot_factor = uniform(-rot_factor, rot_factor)\n",
    "    scale_factor = uniform(1-scale_factor, 1+scale_factor)\n",
    "    trans_factor = [int(imgs.shape[1]*uniform(-trans_factor, trans_factor)),\n",
    "                    int(imgs.shape[2]*uniform(-trans_factor, trans_factor))]\n",
    "\n",
    "    seq = iaa.Sequential([\n",
    "            iaa.Affine(\n",
    "                translate_px={\"x\": trans_factor[0], \"y\": trans_factor[1]},\n",
    "                scale=(scale_factor, scale_factor),\n",
    "                rotate=rot_factor\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    seq_det = seq.to_deterministic()\n",
    "\n",
    "    imgs = seq_det.augment_images(imgs)\n",
    "    masks = seq_det.augment_images(masks)\n",
    "\n",
    "    if flip and uniform(0, 1) > 0.5:\n",
    "        imgs = np.flip(imgs, 2).copy()\n",
    "        masks = np.flip(masks, 2).copy()\n",
    "\n",
    "    return imgs, masks\n",
    "\n",
    "def center_crop(img, width):\n",
    "    y, x = img.shape\n",
    "    x_center = x/2.0\n",
    "    y_center = y/2.0\n",
    "    x_min = int(x_center - width/2.0)\n",
    "    x_max = x_min + width\n",
    "    y_min = int(y_center - width/2.0)\n",
    "    y_max = y_min + width\n",
    "    img_cropped = img[y_min:y_max, x_min: x_max]\n",
    "    return img_cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVkT65l9Cn8d"
   },
   "source": [
    "## Dataset and Dataloader\n",
    "<a id=\"Dataloader\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Kna90MxCn8e"
   },
   "outputs": [],
   "source": [
    "class UrinaryStoneDataset(Dataset):\n",
    "    def __init__(self, is_Train=True, augmentation=True):\n",
    "        super(UrinaryStoneDataset, self).__init__()\n",
    "        global data_root, w_min, w_max, crop_size, input_size, rot_factor, scale_factor, trans_factor, flip\n",
    "\n",
    "        self.dcm_list = sorted(glob(os.path.join(data_root, 'Train' if is_Train else 'Valid', 'DCM', '*.dcm')))\n",
    "        self.len = len(self.dcm_list)\n",
    "        self.augmentation = augmentation\n",
    "        self.is_Train = is_Train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Load Image and Mask\n",
    "        dcm_path = self.dcm_list[index]\n",
    "        filename = os.path.basename(dcm_path).rstrip('.dcm')\n",
    "        mask_path = dcm_path.replace('DCM', 'Label').replace('.dcm', '.png')\n",
    "\n",
    "        img_sitk = sitk.ReadImage(dcm_path)\n",
    "        img = sitk.GetArrayFromImage(img_sitk)[0]\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "\n",
    "        # HU Windowing\n",
    "        img = image_windowing(img, w_min, w_max)\n",
    "\n",
    "        # Center Crop and MINMAX to [0, 255] and Resize\n",
    "        img = center_crop(img, crop_size)\n",
    "        mask = center_crop(mask, crop_size)\n",
    "        \n",
    "        img = image_minmax(img)\n",
    "        \n",
    "        img = cv2.resize(img, (input_size, input_size))\n",
    "        mask = cv2.resize(mask, (input_size, input_size))\n",
    "\n",
    "        # MINMAX to [0, 1]\n",
    "        img = img / 255.\n",
    "\n",
    "        # Mask Binarization (0 or 1)\n",
    "        mask = mask_binarization(mask)\n",
    "\n",
    "        # Add channel axis\n",
    "        img = img[None, ...].astype(np.float32)\n",
    "        mask = mask[None, ...].astype(np.float32)\n",
    "                \n",
    "        # Augmentation\n",
    "        if self.augmentation:\n",
    "            img, mask = augment_imgs_and_masks(img, mask, rot_factor, scale_factor, trans_factor, flip)\n",
    "        \n",
    "        if self.is_Train:\n",
    "            return img, mask\n",
    "        else:\n",
    "            return img, mask, filename\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDMPSqi7Cn8h"
   },
   "outputs": [],
   "source": [
    "# Make Data Loader\n",
    "trn_dataset = UrinaryStoneDataset(is_Train=True, augmentation=True)\n",
    "val_dataset = UrinaryStoneDataset(is_Train=False, augmentation=False)\n",
    "\n",
    "train_dataloader = DataLoader(trn_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=workers)\n",
    "\n",
    "valid_dataloader = DataLoader(val_dataset,\n",
    "                              batch_size=1,\n",
    "                              shuffle=False,\n",
    "                              num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwaPLEhDCn8l"
   },
   "source": [
    "## Optimizer & Loss\n",
    "<a id=\"Optimizer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjMAWK_dCn8m"
   },
   "outputs": [],
   "source": [
    "def compute_per_channel_dice(input, target, epsilon=1e-5, ignore_index=None, weight=None):\n",
    "    # assumes that input is a normalized probability\n",
    "    # input and target shapes must match\n",
    "    assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
    "\n",
    "    # mask ignore_index if present\n",
    "    if ignore_index is not None:\n",
    "        mask = target.clone().ne_(ignore_index)\n",
    "        mask.requires_grad = False\n",
    "\n",
    "        input = input * mask\n",
    "        target = target * mask\n",
    "\n",
    "    input = flatten(input)\n",
    "    target = flatten(target)\n",
    "\n",
    "    # Compute per channel Dice Coefficient\n",
    "    intersect = (input * target).sum(-1)\n",
    "    if weight is not None:\n",
    "        intersect = weight * intersect\n",
    "\n",
    "    denominator = (input + target).sum(-1)\n",
    "    return 2. * intersect / denominator.clamp(min=epsilon)\n",
    "\n",
    "def flatten(tensor):\n",
    "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
    "    The shapes are transformed as follows:\n",
    "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
    "    \"\"\"\n",
    "    C = tensor.size(1)\n",
    "    # new axis order\n",
    "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
    "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
    "    transposed = tensor.permute(axis_order).contiguous()\n",
    "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
    "    return transposed.view(C, -1)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Computes Dice Loss, which just 1 - DiceCoefficient described above.\n",
    "    Additionally allows per-class weights to be provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, weight=None, ignore_index=None, sigmoid_normalization=True,\n",
    "                 skip_last_target=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        if isinstance(weight, list):\n",
    "            weight = torch.Tensor(weight)\n",
    "            \n",
    "        self.epsilon = epsilon\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        if sigmoid_normalization:\n",
    "            self.normalization = nn.Sigmoid()\n",
    "        else:\n",
    "            self.normalization = nn.Softmax(dim=1)\n",
    "        # if True skip the last channel in the target\n",
    "        self.skip_last_target = skip_last_target\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # get probabilities from logits\n",
    "\n",
    "        input = self.normalization(input)\n",
    "        if self.weight is not None:\n",
    "            weight = Variable(self.weight, requires_grad=False).to(input.device)\n",
    "        else:\n",
    "            weight = None\n",
    "\n",
    "        if self.skip_last_target:\n",
    "            target = target[:, :-1, ...]\n",
    "\n",
    "        per_channel_dice = compute_per_channel_dice(input, target, epsilon=self.epsilon, ignore_index=self.ignore_index, weight=weight)\n",
    "        # Average the Dice score across all channels/classes\n",
    "        return torch.mean(1. - per_channel_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89FE5m8eCn8o"
   },
   "outputs": [],
   "source": [
    "criterion = DiceLoss(sigmoid_normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFT7-h9vCn8r"
   },
   "outputs": [],
   "source": [
    "if no_bias_decay:\n",
    "    weight_params = []\n",
    "    bias_params = []\n",
    "    \n",
    "    for n, p in net.named_parameters():\n",
    "        if 'bias' in n:\n",
    "            bias_params.append(p)\n",
    "        else:\n",
    "            weight_params.append(p)\n",
    "    parameters = [{'params' : bias_params, 'weight_decay' : 0},\n",
    "                  {'params' : weight_params}]\n",
    "else:\n",
    "    parameters = net.parameters()\n",
    "\n",
    "optimizer = optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbti4O3dCn8v"
   },
   "source": [
    "## ETC\n",
    "<a id=\"ETC\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5cnAIMxCn8v"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OA9VfHLkCn8y"
   },
   "source": [
    "## Evaluation Metrics\n",
    "<a id=\"EvaluationMetrics\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMPcbW_WCn8y"
   },
   "outputs": [],
   "source": [
    "def get_iou(preds, labels):\n",
    "    SMOOTH = 1e-6\n",
    "\n",
    "    preds = preds.squeeze(1).int()\n",
    "    labels = labels.squeeze(1).int()\n",
    "\n",
    "    intersection = (preds & labels).float().sum((1, 2)) # zero if mask=0 or Prediction=0\n",
    "    union = (preds | labels).float().sum((1, 2)) # zero if both are 0\n",
    "\n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "def avg_precision(iou_list, thresh1=0.5, thresh2=0.75):\n",
    "    # IoU array\n",
    "    iou_array = np.array(iou_list)\n",
    "    \n",
    "    # thresh 1\n",
    "    pred_binary_th1 = np.where(iou_array > thresh1, 1, 0)\n",
    "    \n",
    "    # thresh 2\n",
    "    pred_binary_th2 = np.where(iou_array > thresh2, 1, 0)\n",
    "    \n",
    "    prec_thresh1 = np.sum(pred_binary_th1) / len(pred_binary_th1)\n",
    "    prec_thresh2 = np.sum(pred_binary_th2) / len(pred_binary_th1)\n",
    "    \n",
    "    iou_mean = np.mean(iou_array)\n",
    "    \n",
    "    return prec_thresh1, prec_thresh2, iou_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOLKNgfLCn81"
   },
   "source": [
    "## Learning rate\n",
    "<a id=\"LR\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAspKCxnCn81"
   },
   "outputs": [],
   "source": [
    "def get_current_lr(optimizer):\n",
    "    return optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "def lr_update(epoch, optimizer):\n",
    "    global lr_warmup_epoch, lr_decay_epoch\n",
    "    \n",
    "    prev_lr = get_current_lr(optimizer)\n",
    "    \n",
    "    if 0 <= epoch < lr_warmup_epoch:\n",
    "        mul_rate = 10 ** (1/lr_warmup_epoch)\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= mul_rate\n",
    "\n",
    "        current_lr = get_current_lr(optimizer)\n",
    "        print(\"LR warm-up : %.7f to %.7f\" % (prev_lr, current_lr))\n",
    "\n",
    "    else:\n",
    "        if isinstance(lr_decay_epoch, list):\n",
    "            if (epoch+1) in lr_decay_epoch:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = (prev_lr * 0.1)\n",
    "                    print(\"LR Decay : %.7f to %.7f\" % (prev_lr, prev_lr * 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch6srP8nCn87"
   },
   "source": [
    "## Core functions\n",
    "<a id=\"corefunctions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpqARgdzCn87"
   },
   "outputs": [],
   "source": [
    "def train(net, train_dataloader, optimizer, criterion, epoch, train_writer):\n",
    "    global max_epoch, conf_threshold\n",
    "    \n",
    "    print(\"Start Training...\")\n",
    "    net.train()\n",
    "\n",
    "    losses, total_iou = AverageMeter(), AverageMeter()\n",
    "\n",
    "    for it, (img, mask) in enumerate(train_dataloader):\n",
    "        # Optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Load Data\n",
    "        img, mask = torch.Tensor(img).float(), torch.Tensor(mask).float()\n",
    "        img, mask = img.cuda(non_blocking=True), mask.cuda(non_blocking=True)\n",
    "\n",
    "        # Predict\n",
    "        pred = net(img)\n",
    "\n",
    "        # Loss Calculation\n",
    "        loss = criterion(pred, mask)\n",
    "        \n",
    "        # Backward and step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Convert to Binary\n",
    "        pred = pred.sigmoid()\n",
    "        pred_mask = mask_binarization(pred, conf_threshold)\n",
    "\n",
    "        # Calculation IoU Score\n",
    "        iou_score = get_iou(pred_mask, mask)\n",
    "        total_iou.update(iou_score.mean().item(), img.size(0))\n",
    "\n",
    "        # Stack Results\n",
    "        losses.update(loss.item(), img.size(0))\n",
    "        \n",
    "        # Print Results\n",
    "        print('Epoch[%3d/%3d] | Iter[%3d/%3d] | Loss %.4f | Iou %.4f'\n",
    "            % (epoch+1, max_epoch, it+1, len(train_dataloader), losses.avg, total_iou.avg), end='\\r')\n",
    "\n",
    "    print(\">>> Epoch[%3d/%3d] | Training Loss : %.4f | Iou %.4f\\n \"\n",
    "        % (epoch+1, max_epoch, losses.avg, total_iou.avg))\n",
    "\n",
    "    train_writer.add_scalar(\"train/loss\", losses.avg, epoch+1)\n",
    "    train_writer.add_scalar(\"train/IoU\", total_iou.avg, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNNEEzuHCn8-"
   },
   "outputs": [],
   "source": [
    "def validate(valid_dataloader, net, criterion, epoch, best_iou, best_epoch,train_writer):\n",
    "    global save_dir, max_epoch, ngpu, conf_threshold\n",
    "    \n",
    "    print(\"Start Evaluation...\")\n",
    "    net.eval()\n",
    "\n",
    "    # Result containers\n",
    "    losses, total_iou = AverageMeter(), AverageMeter()\n",
    "\n",
    "    for it, (img, mask, _) in enumerate(valid_dataloader):\n",
    "        # Load Data\n",
    "        img, mask = torch.Tensor(img).float(), torch.Tensor(mask).float()\n",
    "        img, mask = img.cuda(non_blocking=True), mask.cuda(non_blocking=True)\n",
    "\n",
    "        # Predict\n",
    "        pred = net(img)\n",
    "\n",
    "        # Loss Calculation\n",
    "        loss = criterion(pred, mask)\n",
    "\n",
    "        # Convert to Binary\n",
    "        pred = pred.sigmoid()\n",
    "        pred_mask = mask_binarization(pred, conf_threshold)\n",
    "\n",
    "        # Calculation IoU Score\n",
    "        iou_score = get_iou(pred_mask, mask)\n",
    "        total_iou.update(iou_score.mean().item(), img.size(0))\n",
    "\n",
    "        # Stack Results\n",
    "        losses.update(loss.item(), img.size(0))\n",
    "\n",
    "    print(\">>> Epoch[%3d/%3d] | Test Loss : %.4f | Iou %.4f\"\n",
    "        % (epoch+1, max_epoch, losses.avg, total_iou.avg))\n",
    "\n",
    "    train_writer.add_scalar(\"valid/loss\", losses.avg, epoch+1)\n",
    "    train_writer.add_scalar(\"valid/IoU\", total_iou.avg, epoch+1)\n",
    "\n",
    "    # Update Result\n",
    "    if total_iou.avg > best_iou:\n",
    "        print('Best Score Updated...')\n",
    "        best_iou = total_iou.avg\n",
    "        best_epoch = epoch\n",
    "\n",
    "        model_filename = '%s/epoch_%04d_iou_%.4f_loss_%.8f.pth' % (save_dir, epoch+1, best_iou, losses.avg)\n",
    "\n",
    "        # Single GPU\n",
    "        if ngpu == 1:\n",
    "            torch.save(net.state_dict(), model_filename)\n",
    "        # Multi GPU\n",
    "        else:\n",
    "            torch.save(net.module.state_dict(), model_filename)\n",
    "\n",
    "    print('>>> Current best: IoU: %.8f in %3d epoch\\n' % (best_iou, best_epoch+1))\n",
    "    \n",
    "    return best_iou, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYjl47evCn9B"
   },
   "outputs": [],
   "source": [
    "def evaluate(valid_dataloader, net):\n",
    "    global conf_threshold, save_dir\n",
    "    \n",
    "    print(\"Start Evaluation...\")\n",
    "    net.eval()\n",
    "\n",
    "    iou_dict = dict()\n",
    "    for idx, (img, mask, filename) in enumerate(valid_dataloader):\n",
    "        # Load Data\n",
    "        img, mask = torch.Tensor(img).float(), torch.Tensor(mask).float()\n",
    "        img, mask = img.cuda(non_blocking=True), mask.cuda(non_blocking=True)\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            pred = net(img)\n",
    "\n",
    "        # Convert to Binary\n",
    "        pred = pred.sigmoid()\n",
    "        pred_mask = mask_binarization(pred, conf_threshold)\n",
    "\n",
    "        # Calculation IoU Score\n",
    "        iou_score = get_iou(pred_mask, mask)\n",
    "        \n",
    "        for name, iou in zip(filename, list(iou_score.cpu().numpy())):\n",
    "            iou_dict[name] = iou\n",
    "        \n",
    "        # Print log\n",
    "        print(\"{}/{} - IoU {:.4f}\".format(idx+1, len(valid_dataloader), iou_score.item()), end='\\r')\n",
    "    \n",
    "    # Calcuate AP\n",
    "    iou_scores = list(iou_dict.values())\n",
    "    prec_thresh1, prec_thresh2, iou_mean = avg_precision(iou_scores)\n",
    "    print(\"\\n\\n>>> AP50 : %.4f | AP75 : %.4f | mIoU : %.4f\"%(prec_thresh1, prec_thresh2, iou_mean))\n",
    "    \n",
    "    # Dictionary to Dataframe\n",
    "    iou_df = pd.DataFrame.from_dict(iou_dict, orient='index')\n",
    "    iou_df.index.name = 'ID'\n",
    "    iou_df.columns = ['IoU']\n",
    "    \n",
    "    return iou_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piKX3w_ICn9F"
   },
   "source": [
    "# Main code\n",
    "<a id=\"main\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmZXiEIGCn9G"
   },
   "source": [
    "## Train\n",
    "<a id=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNwBtW7VCn9G"
   },
   "outputs": [],
   "source": [
    "debugging = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1qUHAJ0Cn9J",
    "outputId": "631a9347-a317-41f1-f976-d3b6cd4c971b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      ">>> Epoch[  1/150] | Training Loss : 0.9945 | Iou 0.03133\n",
      " \n",
      "Start Evaluation...\n",
      ">>> Epoch[  1/150] | Test Loss : 0.9762 | Iou 0.2324\n",
      "Best Score Updated...\n",
      ">>> Current best: IoU: 0.23244944 in   1 epoch\n",
      "\n",
      "Training done\n"
     ]
    }
   ],
   "source": [
    "# Tensorboard\n",
    "train_writer = SummaryWriter(os.path.join(save_dir, 'logs/'))\n",
    "\n",
    "# Initial Best Score\n",
    "best_iou, best_epoch = [0, 0]\n",
    "\n",
    "for epoch in range(start_epoch, max_epoch):\n",
    "    # Train\n",
    "    train(net, train_dataloader, optimizer, criterion, epoch, train_writer)\n",
    "\n",
    "    # Evaluate\n",
    "    best_iou, best_epoch = validate(valid_dataloader, net, criterion, epoch, best_iou, best_epoch,train_writer)\n",
    "\n",
    "    lr_update(epoch, optimizer)\n",
    "    \n",
    "    if debugging:\n",
    "        break\n",
    "\n",
    "print('Training done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KP-_f26Cn9M"
   },
   "source": [
    "## Evaluation\n",
    "<a id=\"evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxWiK2NNCn9N",
    "outputId": "fead7e25-5bc1-4e08-8f2e-85a11b920a29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained weights\n",
    "best_weights = torch.load('../input/best-weights/epoch_0151_iou_0.7545_loss_0.15650385.pth')\n",
    "net.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnL1l_VUCn9T",
    "outputId": "fd9a0f2f-4706-4ea8-faca-d53885dadcdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Evaluation...\n",
      "100/100 - IoU 0.0976\n",
      "\n",
      ">>> AP50 : 0.7700 | AP75 : 0.1400 | mIoU : 0.5812\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IoU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>0.854545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>0.696970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IoU\n",
       "ID           \n",
       "601  0.854545\n",
       "602  0.696970\n",
       "603  0.833333\n",
       "604  0.533333\n",
       "605  0.666667"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_df = evaluate(valid_dataloader, net)\n",
    "iou_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSN4w6OXCn9Z"
   },
   "outputs": [],
   "source": [
    "iou_df.to_excel('validation_IoU_result.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuiT8ZIXCn9c"
   },
   "source": [
    "## Inference\n",
    "<a id=\"Inference\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmnKhs63Cn9c"
   },
   "outputs": [],
   "source": [
    "test_path_list = glob(os.path.join(data_root, 'Test', 'DCM', '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wlugfisCn9e"
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(save_dir, 'test_pred_labels'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpPZ6Mf0Cn9h"
   },
   "outputs": [],
   "source": [
    "for test_path in test_path_list:\n",
    "    filename = os.path.basename(test_path).rstrip('.dcm')\n",
    "    \n",
    "    # Load Image\n",
    "    img_sitk = sitk.ReadImage(test_path)\n",
    "    img = sitk.GetArrayFromImage(img_sitk)[0]\n",
    "\n",
    "    # HU Windowing\n",
    "    img = image_windowing(img, w_min, w_max)\n",
    "\n",
    "    # Center Crop and MINMAX to [0, 255] and Resize\n",
    "    img = center_crop(img, crop_size)\n",
    "    img = image_minmax(img)\n",
    "    img = cv2.resize(img, (input_size, input_size))\n",
    "\n",
    "    # MINMAX to [0, 1]\n",
    "    img = img / 255.\n",
    "    \n",
    "    # Image to Tensor\n",
    "    img_input = torch.Tensor(img[None, None]).float()\n",
    "    img_input = img_input.cuda(non_blocking=True)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        pred = net(img_input)\n",
    "\n",
    "    # Convert to Binary\n",
    "    pred = pred.sigmoid()\n",
    "    pred_mask = mask_binarization(pred, conf_threshold)\n",
    "    \n",
    "    # Save prediction mask\n",
    "    pred_mask = pred_mask[0,0].cpu().numpy().astype(np.uint8)\n",
    "    pred_mask = cv2.cvtColor(pred_mask*255, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.imwrite(os.path.join(save_dir, 'test_pred_labels', '%s.png'%filename), pred_mask)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prn6v_a2Cn9m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tailab-notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
