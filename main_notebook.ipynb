{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urinary-Stone-Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides everything necessary to train and evaluate a urinary stone segmentation model.\n",
    "The baseline network is Modified-UNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from random import uniform\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from glob import glob\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from tqdm import tqdm\n",
    "from medpy.metric.binary import sensitivity, specificity, dc, hd95\n",
    "\n",
    "from options import parse_option\n",
    "from network import create_model\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from matplotlib import pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.fastest = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified-Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modified2DUNet(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_classes, base_n_filter = 8):\n",
    "\t\tsuper(Modified2DUNet, self).__init__()\n",
    "\t\tself.in_channels = in_channels\n",
    "\t\tself.n_classes = n_classes\n",
    "\t\tself.base_n_filter = base_n_filter\n",
    "\n",
    "\t\tself.lrelu = nn.LeakyReLU()\n",
    "\t\tself.dropout3d = nn.Dropout3d(p=0.6)\n",
    "\t\t\n",
    "\n",
    "\t\t# Level 1 context pathway\n",
    "\t\tself.conv3d_c1_1 = nn.Conv2d(self.in_channels, self.base_n_filter, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\t\tself.conv3d_c1_2 = nn.Conv2d(self.base_n_filter, self.base_n_filter, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\t\tself.lrelu_conv_c1 = self.lrelu_conv(self.base_n_filter, self.base_n_filter)\n",
    "\t\tself.gnorm3d_c1 = nn.GroupNorm(self.base_n_filter//2, self.base_n_filter)\n",
    "\n",
    "\t\t# Level 2 context pathway\n",
    "\t\tself.conv3d_c2 = nn.Conv2d(self.base_n_filter, self.base_n_filter*2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\t\tself.norm_lrelu_conv_c2 = self.norm_lrelu_conv(self.base_n_filter*2, self.base_n_filter*2)\n",
    "\t\tself.norm_lrelu_conv_c2 = self.norm_lrelu_conv(self.base_n_filter*2, self.base_n_filter*2)\n",
    "\t\tself.norm_lrelu_conv_c2 = self.norm_lrelu_conv(self.base_n_filter*2, self.base_n_filter*2)\n",
    "\t\tself.gnorm3d_c2 = nn.GroupNorm(self.base_n_filter, self.base_n_filter*2)\n",
    "\n",
    "\t\t# Level 3 context pathway\n",
    "\t\tself.conv3d_c3 = nn.Conv2d(self.base_n_filter*2, self.base_n_filter*4, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\t\tself.norm_lrelu_conv_c3 = self.norm_lrelu_conv(self.base_n_filter*4, self.base_n_filter*4)\n",
    "\t\tself.norm_lrelu_conv_c3 = self.norm_lrelu_conv(self.base_n_filter*4, self.base_n_filter*4)\n",
    "\t\tself.norm_lrelu_conv_c3 = self.norm_lrelu_conv(self.base_n_filter*4, self.base_n_filter*4)\n",
    "\t\tself.gnorm3d_c3 = nn.GroupNorm(self.base_n_filter, self.base_n_filter*4)\n",
    "\n",
    "\t\t# Level 4 context pathway\n",
    "\t\tself.conv3d_c4 = nn.Conv2d(self.base_n_filter*4, self.base_n_filter*8, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\t\tself.norm_lrelu_conv_c4 = self.norm_lrelu_conv(self.base_n_filter*8, self.base_n_filter*8)\n",
    "\t\tself.norm_lrelu_conv_c4 = self.norm_lrelu_conv(self.base_n_filter*8, self.base_n_filter*8)\n",
    "\t\tself.norm_lrelu_conv_c4 = self.norm_lrelu_conv(self.base_n_filter*8, self.base_n_filter*8)\n",
    "\t\tself.gnorm3d_c4 = nn.GroupNorm(self.base_n_filter*2, self.base_n_filter*8)\n",
    "\n",
    "\t\t# Level 5 context pathway, level 0 localization pathway\n",
    "\t\tself.conv3d_c5 = nn.Conv2d(self.base_n_filter*8, self.base_n_filter*16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\t\tself.norm_lrelu_conv_c5 = self.norm_lrelu_conv(self.base_n_filter*16, self.base_n_filter*16)\n",
    "\t\tself.norm_lrelu_conv_c5 = self.norm_lrelu_conv(self.base_n_filter*16, self.base_n_filter*16)\n",
    "\t\tself.norm_lrelu_conv_c5 = self.norm_lrelu_conv(self.base_n_filter*16, self.base_n_filter*16)\n",
    "\t\tself.norm_lrelu_upscale_conv_norm_lrelu_l0_1 = self.norm_lrelu_upscale_conv_norm_lrelu_1(self.base_n_filter*16)\n",
    "\t\tself.norm_lrelu_upscale_conv_norm_lrelu_l0_2 = self.norm_lrelu_upscale_conv_norm_lrelu_2(self.base_n_filter*16, self.base_n_filter*8)\n",
    "\n",
    "\t\tself.conv3d_l0 = nn.Conv2d(self.base_n_filter*8, self.base_n_filter*8, kernel_size = 1, stride=1, padding=0, bias=False)\n",
    "\t\tself.gnorm3d_l0 = nn.GroupNorm(self.base_n_filter*2, self.base_n_filter*8)\n",
    "\n",
    "\t\t# Level 1 localization pathway\n",
    "\t\tself.conv_norm_lrelu_l1 = self.conv_norm_lrelu(self.base_n_filter*16, self.base_n_filter*16)\n",
    "\t\tself.conv3d_l1 = nn.Conv2d(self.base_n_filter*16, self.base_n_filter*8, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\t\tself.norm_lrelu_upscale_conv_norm_lrelu_l1_1 = self.norm_lrelu_upscale_conv_norm_lrelu_1(self.base_n_filter*8)\n",
    "\t\tself.norm_lrelu_upscale_conv_norm_lrelu_l1_2 = self.norm_lrelu_upscale_conv_norm_lrelu_2(self.base_n_filter*8, self.base_n_filter*4)\n",
    "\n",
    "\t\t# Level 2 localization pathway\n",
    "\t\tself.conv_norm_lrelu_l2 = self.conv_norm_lrelu(self.base_n_filter*8, self.base_n_filter*8)\n",
    "\t\tself.conv3d_l2 = nn.Conv2d(self.base_n_filter*8, self.base_n_filter*4, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\t\tself.norm_lrelu_upscale_conv_norm_lrelu_l2_1 = self.norm_lrelu_upscale_conv_norm_lrelu_1(self.base_n_filter*4)\n",
    "\t\tself.norm_lrelu_upscale_conv_norm_lrelu_l2_2 = self.norm_lrelu_upscale_conv_norm_lrelu_2(self.base_n_filter*4, self.base_n_filter*2)\n",
    "\n",
    "\t\t# Level 3 localization pathway\n",
    "\t\tself.conv_norm_lrelu_l3 = self.conv_norm_lrelu(self.base_n_filter*4, self.base_n_filter*4)\n",
    "\t\tself.conv3d_l3 = nn.Conv2d(self.base_n_filter*4, self.base_n_filter*2, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\t\tself.norm_lrelu_upscale_conv_norm_lrelu_l3_1 = self.norm_lrelu_upscale_conv_norm_lrelu_1(self.base_n_filter*2)\n",
    "\t\tself.norm_lrelu_upscale_conv_norm_lrelu_l3_2 = self.norm_lrelu_upscale_conv_norm_lrelu_2(self.base_n_filter*2, self.base_n_filter)\n",
    "\n",
    "\t\t# Level 4 localization pathway\n",
    "\t\tself.conv_norm_lrelu_l4 = self.conv_norm_lrelu(self.base_n_filter*2, self.base_n_filter*2)\n",
    "\t\tself.conv3d_l4 = nn.Conv2d(self.base_n_filter*2, self.n_classes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "\t\tself.ds2_1x1_conv3d = nn.Conv2d(self.base_n_filter*8, self.n_classes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\t\tself.ds3_1x1_conv3d = nn.Conv2d(self.base_n_filter*4, self.n_classes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tdef conv_norm_lrelu(self, feat_in, feat_out):\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\tnn.Conv2d(feat_in, feat_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "\t\t\tnn.GroupNorm(feat_out//2, feat_out),\n",
    "\t\t\tnn.LeakyReLU())\n",
    "\n",
    "\tdef norm_lrelu_conv(self, feat_in, feat_out):\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\tnn.GroupNorm(feat_in//2, feat_in),\n",
    "\t\t\tnn.LeakyReLU(),\n",
    "\t\t\tnn.Conv2d(feat_in, feat_out, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "\n",
    "\tdef lrelu_conv(self, feat_in, feat_out):\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\tnn.LeakyReLU(),\n",
    "\t\t\tnn.Conv2d(feat_in, feat_out, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "\n",
    "\tdef norm_lrelu_upscale_conv_norm_lrelu_1(self, feat_in):\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\tnn.GroupNorm(feat_in//2, feat_in),\n",
    "\t\t\tnn.LeakyReLU())\n",
    "\n",
    "\tdef norm_lrelu_upscale_conv_norm_lrelu_2(self, feat_in, feat_out):\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\t# should be feat_in*2 or feat_in\n",
    "\t\t\tnn.Conv2d(feat_in, feat_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "\t\t\tnn.GroupNorm(feat_out//2, feat_out),\n",
    "\t\t\tnn.LeakyReLU())\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t#  Level 1 context pathway\n",
    "\t\tout = self.conv3d_c1_1(x)\n",
    "\t\tresidual_1 = out\n",
    "\t\tout = self.lrelu(out)\n",
    "\t\tout = self.conv3d_c1_2(out)\n",
    "\t\tout = self.dropout3d(out)\n",
    "\t\tout = self.lrelu_conv_c1(out)\n",
    "\t\t# Element Wise Summation\n",
    "\t\tout += residual_1\n",
    "\t\tcontext_1 = self.lrelu(out)\n",
    "\t\tout = self.gnorm3d_c1(out)\n",
    "\t\tout = self.lrelu(out)\n",
    "\n",
    "\t\t# Level 2 context pathway\n",
    "\t\tout = self.conv3d_c2(out)\n",
    "\t\tresidual_2 = out\n",
    "\t\tout = self.norm_lrelu_conv_c2(out)\n",
    "\t\tout = self.norm_lrelu_conv_c2(out)\n",
    "\t\tout = self.dropout3d(out)\n",
    "\t\tout = self.norm_lrelu_conv_c2(out)\n",
    "\t\tout = self.norm_lrelu_conv_c2(out)\n",
    "\t\tout += residual_2\n",
    "\t\tout = self.gnorm3d_c2(out)\n",
    "\t\tout = self.lrelu(out)\n",
    "\t\tcontext_2 = out\n",
    "\n",
    "\t\t# Level 3 context pathway\n",
    "\t\tout = self.conv3d_c3(out)\n",
    "\t\tresidual_3 = out\n",
    "\t\tout = self.norm_lrelu_conv_c3(out)\n",
    "\t\tout = self.norm_lrelu_conv_c3(out)\n",
    "\t\tout = self.dropout3d(out)\n",
    "\t\tout = self.norm_lrelu_conv_c3(out)\n",
    "\t\tout = self.norm_lrelu_conv_c3(out)\n",
    "\t\tout += residual_3\n",
    "\t\tout = self.gnorm3d_c3(out)\n",
    "\t\tout = self.lrelu(out)\n",
    "\t\tcontext_3 = out\n",
    "\n",
    "\t\t# Level 4 context pathway\n",
    "\t\tout = self.conv3d_c4(out)\n",
    "\t\tresidual_4 = out\n",
    "\t\tout = self.norm_lrelu_conv_c4(out)\n",
    "\t\tout = self.norm_lrelu_conv_c4(out)\n",
    "\t\tout = self.dropout3d(out)\n",
    "\t\tout = self.norm_lrelu_conv_c4(out)\n",
    "\t\tout = self.norm_lrelu_conv_c4(out)\n",
    "\t\tout += residual_4\n",
    "\t\tout = self.gnorm3d_c4(out)\n",
    "\t\tout = self.lrelu(out)\n",
    "\t\tcontext_4 = out\n",
    "\n",
    "\t\t# Level 5\n",
    "\t\tout = self.conv3d_c5(out)\n",
    "\t\tresidual_5 = out\n",
    "\t\tout = self.norm_lrelu_conv_c5(out)\n",
    "\t\tout = self.norm_lrelu_conv_c5(out)\n",
    "\t\tout = self.dropout3d(out)\n",
    "\t\tout = self.norm_lrelu_conv_c5(out)\n",
    "\t\tout = self.norm_lrelu_conv_c5(out)\n",
    "\t\tout += residual_5\n",
    "\t\tout = self.norm_lrelu_upscale_conv_norm_lrelu_l0_1(out)\n",
    "\t\tout = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "\t\tout = self.norm_lrelu_upscale_conv_norm_lrelu_l0_2(out)\n",
    "\n",
    "\t\tout = self.conv3d_l0(out)\n",
    "\t\tout = self.gnorm3d_l0(out)\n",
    "\t\tout = self.lrelu(out)\n",
    "\n",
    "\t\t# Level 1 localization pathway\n",
    "\t\tout = F.interpolate(out, size = context_4.size()[-2:])\n",
    "\t\tout = torch.cat([out, context_4], dim=1)\n",
    "\t\tout = self.conv_norm_lrelu_l1(out)\n",
    "\t\tout = self.conv3d_l1(out)\n",
    "\t\tout = self.norm_lrelu_upscale_conv_norm_lrelu_l1_1(out)\n",
    "\t\tout = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "\t\tout = self.norm_lrelu_upscale_conv_norm_lrelu_l1_2(out)\n",
    "\n",
    "\n",
    "\t\t# Level 2 localization pathway\n",
    "\t\tout = F.interpolate(out, size = context_3.size()[-2:])\n",
    "\t\tout = torch.cat([out, context_3], dim=1)\n",
    "\t\tout = self.conv_norm_lrelu_l2(out)\n",
    "\t\tds2 = out\n",
    "\t\tout = self.conv3d_l2(out)\n",
    "\t\tout = self.norm_lrelu_upscale_conv_norm_lrelu_l2_1(out)\n",
    "\t\tout = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "\t\tout = self.norm_lrelu_upscale_conv_norm_lrelu_l2_2(out)\n",
    "\n",
    "\t\t# Level 3 localization pathway\n",
    "\t\tout = F.interpolate(out, size = context_2.size()[-2:])\n",
    "\t\tout = torch.cat([out, context_2], dim=1)\n",
    "\t\tout = self.conv_norm_lrelu_l3(out)\n",
    "\t\tds3 = out\n",
    "\t\tout = self.conv3d_l3(out)\n",
    "\t\tout = self.norm_lrelu_upscale_conv_norm_lrelu_l3_1(out)\n",
    "\t\tout = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "\t\tout = self.norm_lrelu_upscale_conv_norm_lrelu_l3_2(out)\n",
    "\n",
    "\t\t# Level 4 localization pathway\n",
    "\t\tout = F.interpolate(out, size = context_1.size()[-2:])\n",
    "\t\tout = torch.cat([out, context_1], dim=1)\n",
    "\t\tout = self.conv_norm_lrelu_l4(out)\n",
    "\t\tout_pred = self.conv3d_l4(out)\n",
    "\n",
    "\t\tds2_1x1_conv = self.ds2_1x1_conv3d(ds2)\n",
    "\t\tds1_ds2_sum_upscale = F.interpolate(ds2_1x1_conv, scale_factor=2, mode='nearest')\n",
    "\t\tds3_1x1_conv = self.ds3_1x1_conv3d(ds3)\n",
    "\t\tds1_ds2_sum_upscale = F.interpolate(ds1_ds2_sum_upscale, size = ds3_1x1_conv.size()[-2:])\n",
    "\t\tds1_ds2_sum_upscale_ds3_sum = ds1_ds2_sum_upscale + ds3_1x1_conv\n",
    "\t\tds1_ds2_sum_upscale_ds3_sum_upscale = F.interpolate(ds1_ds2_sum_upscale_ds3_sum, scale_factor=2, mode='nearest')\n",
    "\n",
    "\t\tout = out_pred + ds1_ds2_sum_upscale_ds3_sum_upscale\n",
    "\t\t# out = out.permute(0, 2, 3, 4, 1).contiguous().view(-1, self.n_classes)\n",
    "\t\t# out = out.view(-1, self.n_classes)\n",
    "\t\t\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(opt):\n",
    "    # Load network\n",
    "    net = Modified2DUNet(1, 1, opt.base_n_filter)\n",
    "\n",
    "    # GPU settings\n",
    "    if opt.use_gpu:\n",
    "        net.cuda()\n",
    "        if opt.ngpu > 1:\n",
    "            net = torch.nn.DataParallel(net)\n",
    "    \n",
    "    if opt.resume:\n",
    "        if os.path.isfile(opt.exp + \"/\" + opt.resume):\n",
    "            pretrained_dict = torch.load(opt.exp + \"/\" + opt.resume, map_location=torch.device('cpu'))\n",
    "            model_dict = net.state_dict()\n",
    "\n",
    "            match_cnt = 0\n",
    "            mismatch_cnt = 0\n",
    "            pretrained_dict_matched = dict()\n",
    "            for k, v in pretrained_dict.items():\n",
    "                if k in model_dict and v.size() == model_dict[k].size():\n",
    "                    pretrained_dict_matched[k] = v\n",
    "                    match_cnt += 1\n",
    "                else:\n",
    "                    mismatch_cnt += 1\n",
    "                    \n",
    "            model_dict.update(pretrained_dict_matched) \n",
    "            net.load_state_dict(model_dict)\n",
    "\n",
    "            print(\"=> Successfully loaded weights from %s (%d matched / %d mismatched)\" % (opt.resume, match_cnt, mismatch_cnt))\n",
    "\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(opt.resume))\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_windowing(img, w_min=0, w_max=300):\n",
    "    img_w = img.copy()\n",
    "\n",
    "    img_w[img_w < w_min] = w_min\n",
    "    img_w[img_w > w_max] = w_max\n",
    "\n",
    "    return img_w\n",
    "    \n",
    "def image_minmax(img):\n",
    "    img_minmax = ((img - np.min(img)) / (np.max(img) - np.min(img))).copy()\n",
    "    img_minmax = (img_minmax * 255).astype(np.uint8)\n",
    "        \n",
    "    return img_minmax\n",
    "\n",
    "def mask_binarization(mask_array):\n",
    "    threshold = np.max(mask_array) / 2\n",
    "    mask_binarized = (mask_array > threshold).astype(np.uint8)\n",
    "    \n",
    "    return mask_binarized\n",
    "\n",
    "def augment_imgs_and_masks(imgs, masks, rot_factor, scale_factor, trans_factor, flip):\n",
    "    rot_factor = uniform(-rot_factor, rot_factor)\n",
    "    scale_factor = uniform(1-scale_factor, 1+scale_factor)\n",
    "    trans_factor = [int(imgs.shape[1]*uniform(-trans_factor, trans_factor)),\n",
    "                    int(imgs.shape[2]*uniform(-trans_factor, trans_factor))]\n",
    "\n",
    "    seq = iaa.Sequential([\n",
    "            iaa.Affine(\n",
    "                translate_px={\"x\": trans_factor[0], \"y\": trans_factor[1]},\n",
    "                scale=(scale_factor, scale_factor),\n",
    "                rotate=rot_factor\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    seq_det = seq.to_deterministic()\n",
    "\n",
    "    imgs = seq_det.augment_images(imgs)\n",
    "    masks = seq_det.augment_images(masks)\n",
    "\n",
    "    if flip and uniform(0, 1) > 0.5:\n",
    "        imgs = np.flip(imgs, 2).copy()\n",
    "        masks = np.flip(masks, 2).copy()\n",
    "\n",
    "    return imgs, masks\n",
    "\n",
    "\n",
    "def mask_binarization(mask_array):\n",
    "    threshold = np.max(mask_array) / 2\n",
    "    mask_binarized = (mask_array > threshold).astype(np.uint8)\n",
    "    \n",
    "    return mask_binarized\n",
    "\n",
    "\n",
    "def center_crop(img, width):\n",
    "    y, x = img.shape\n",
    "    x_center = x/2.0\n",
    "    y_center = y/2.0\n",
    "    x_min = int(x_center - width/2.0)\n",
    "    x_max = x_min + width\n",
    "    y_min = int(y_center - width/2.0)\n",
    "    y_max = y_min + width\n",
    "    img_cropped = img[y_min:y_max, x_min: x_max]\n",
    "    return img_cropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset & loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UrinaryStoneDataset\n",
    "class UrinaryStoneDataset(Dataset):\n",
    "    def __init__(self, opt, is_Train=True, augmentation=True):\n",
    "        super(UrinaryStoneDataset, self).__init__()\n",
    "\n",
    "        self.dcm_list = sorted(glob(os.path.join(opt.data_root, 'Train' if is_Train else 'Valid', 'DCM', '*.dcm')))\n",
    "        \n",
    "        self.len = len(self.dcm_list)\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.opt = opt\n",
    "\n",
    "        self.is_Train = is_Train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load Image and Mask\n",
    "        dcm_path = self.dcm_list[index]\n",
    "        mask_path = dcm_path.replace('DCM', 'Label').replace('.dcm', '.png')\n",
    "\n",
    "        img_sitk = sitk.ReadImage(dcm_path)\n",
    "        img = sitk.GetArrayFromImage(img_sitk)[0]\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "\n",
    "        # HU Windowing\n",
    "        img = image_windowing(img, self.opt.w_min, self.opt.w_max)\n",
    "\n",
    "        # Center Crop and MINMAX to [0, 255] and Resize\n",
    "        img = center_crop(img, self.opt.crop_size)\n",
    "        mask = center_crop(mask, self.opt.crop_size)\n",
    "        \n",
    "        img = image_minmax(img)\n",
    "        \n",
    "        img = cv2.resize(img, (self.opt.input_size, self.opt.input_size))\n",
    "        mask = cv2.resize(mask, (self.opt.input_size, self.opt.input_size))\n",
    "\n",
    "        # MINMAX to [0, 1]\n",
    "        img = img / 255.\n",
    "\n",
    "        # Mask Binarization (0 or 1)\n",
    "        mask = mask_binarization(mask)\n",
    "\n",
    "        # Add channel axis\n",
    "        img = img[None, ...].astype(np.float32)\n",
    "        mask = mask[None, ...].astype(np.float32)\n",
    "                \n",
    "        # Augmentation\n",
    "        if self.augmentation:\n",
    "            img, mask = augment_imgs_and_masks(img, mask, self.opt.rot_factor, self.opt.scale_factor, self.opt.trans_factor, self.opt.flip)\n",
    "\n",
    "        return img, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dataloader\n",
    "def get_dataloader(opt):\n",
    "    trn_dataset = UrinaryStoneDataset(opt, is_Train=True, augmentation=True)\n",
    "    val_dataset = UrinaryStoneDataset(opt, is_Train=False, augmentation=False)\n",
    "\n",
    "    train_dataloader = DataLoader(trn_dataset,\n",
    "                                  batch_size=opt.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=opt.workers)\n",
    "\n",
    "    valid_dataloader = DataLoader(val_dataset,\n",
    "                                  batch_size=opt.batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=opt.workers)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer & Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Computes Dice Loss, which just 1 - DiceCoefficient described above.\n",
    "    Additionally allows per-class weights to be provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, weight=None, ignore_index=None, sigmoid_normalization=True,\n",
    "                 skip_last_target=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        if isinstance(weight, list):\n",
    "            weight = torch.Tensor(weight)\n",
    "            \n",
    "        self.epsilon = epsilon\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        if sigmoid_normalization:\n",
    "            self.normalization = nn.Sigmoid()\n",
    "        else:\n",
    "            self.normalization = nn.Softmax(dim=1)\n",
    "        # if True skip the last channel in the target\n",
    "        self.skip_last_target = skip_last_target\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # get probabilities from logits\n",
    "\n",
    "        input = self.normalization(input)\n",
    "        if self.weight is not None:\n",
    "            weight = Variable(self.weight, requires_grad=False).to(input.device)\n",
    "        else:\n",
    "            weight = None\n",
    "\n",
    "        if self.skip_last_target:\n",
    "            target = target[:, :-1, ...]\n",
    "\n",
    "        per_channel_dice = compute_per_channel_dice(input, target, epsilon=self.epsilon, ignore_index=self.ignore_index, weight=weight)\n",
    "        # Average the Dice score across all channels/classes\n",
    "        return torch.mean(1. - per_channel_dice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_per_channel_dice(input, target, epsilon=1e-5, ignore_index=None, weight=None):\n",
    "    # assumes that input is a normalized probability\n",
    "    # input and target shapes must match\n",
    "    assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
    "\n",
    "    # mask ignore_index if present\n",
    "    if ignore_index is not None:\n",
    "        mask = target.clone().ne_(ignore_index)\n",
    "        mask.requires_grad = False\n",
    "\n",
    "        input = input * mask\n",
    "        target = target * mask\n",
    "\n",
    "    input = flatten(input)\n",
    "    target = flatten(target)\n",
    "\n",
    "    # Compute per channel Dice Coefficient\n",
    "    intersect = (input * target).sum(-1)\n",
    "    if weight is not None:\n",
    "        intersect = weight * intersect\n",
    "\n",
    "    denominator = (input + target).sum(-1)\n",
    "    return 2. * intersect / denominator.clamp(min=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(tensor):\n",
    "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
    "    The shapes are transformed as follows:\n",
    "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
    "    \"\"\"\n",
    "    C = tensor.size(1)\n",
    "    # new axis order\n",
    "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
    "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
    "    transposed = tensor.permute(axis_order).contiguous()\n",
    "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
    "    return transposed.view(C, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, apply_nonlin=None, batch_dice=False, do_bg=True, smooth=1.,\n",
    "                 square=False):\n",
    "        \"\"\"\n",
    "        paper: https://link.springer.com/chapter/10.1007/978-3-319-50835-1_22\n",
    "        \n",
    "        \"\"\"\n",
    "        super(IoULoss, self).__init__()\n",
    "\n",
    "        self.square = square\n",
    "        self.do_bg = do_bg\n",
    "        self.batch_dice = batch_dice\n",
    "        self.apply_nonlin = apply_nonlin\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, x, y, loss_mask=None):\n",
    "        shp_x = x.shape\n",
    "\n",
    "        if self.batch_dice:\n",
    "            axes = [0] + list(range(2, len(shp_x)))\n",
    "        else:\n",
    "            axes = list(range(2, len(shp_x)))\n",
    "\n",
    "        if self.apply_nonlin is not None:\n",
    "            x = self.apply_nonlin(x)\n",
    "\n",
    "        tp, fp, fn = get_tp_fp_fn(x, y, axes, loss_mask, self.square)\n",
    "\n",
    "\n",
    "        iou = (tp + self.smooth) / (tp + fp + fn + self.smooth)\n",
    "\n",
    "        if not self.do_bg:\n",
    "            if self.batch_dice:\n",
    "                iou = iou[1:]\n",
    "            else:\n",
    "                iou = iou[:, 1:]\n",
    "        iou = iou.mean()\n",
    "\n",
    "        return -iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(opt):\n",
    "    if opt.loss == 'dice':\n",
    "        loss = DiceLoss(sigmoid_normalization=True)\n",
    "    elif opt.loss.lower() == 'IoU':\n",
    "        loss = IoULoss()\n",
    "    else:\n",
    "        raise ValueError(\"Only 'dice' loss is supported now.\")\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(net, opt):\n",
    "  if isinstance(net, list):\n",
    "    optims = []\n",
    "    for network in net:\n",
    "      optims.append(get_optimizer(network, opt))\n",
    "    return optims\n",
    "\n",
    "  else:\n",
    "    if opt.no_bias_decay:\n",
    "      weight_params = []\n",
    "      bias_params = []\n",
    "      for n, p in net.named_parameters():\n",
    "          if 'bias' in n:\n",
    "              bias_params.append(p)\n",
    "          else:\n",
    "              weight_params.append(p)\n",
    "      parameters = [{'params' : bias_params, 'weight_decay' : 0},\n",
    "                    {'params' : weight_params}]\n",
    "    else:\n",
    "      parameters = net.parameters()\n",
    "\n",
    "    if opt.optim.lower() == 'rmsprop':\n",
    "      optimizer = optim.RMSprop(parameters, lr=opt.lr, momentum=opt.momentum, weight_decay=opt.wd)\n",
    "    elif opt.optim.lower() == 'sgd':\n",
    "      optimizer = optim.SGD(parameters, lr=opt.lr, momentum=opt.momentum, weight_decay=opt.wd)\n",
    "    elif opt.optim.lower() == 'adam':\n",
    "      optimizer = optim.Adam(parameters, lr=opt.lr)\n",
    " \n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "  \"\"\"Computes and stores the average and current value\"\"\"\n",
    "  def __init__(self):\n",
    "      self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.val = 0\n",
    "    self.avg = 0\n",
    "    self.sum = 0\n",
    "    self.count = 0\n",
    "\n",
    "  def update(self, val, n=1):\n",
    "    self.val = val\n",
    "    self.sum += val * n\n",
    "    self.count += n\n",
    "    self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iou_modified(preds, labels, opt):\n",
    "    \n",
    "    SMOOTH = opt.iou_smooth\n",
    "\n",
    "    preds = preds.squeeze(1).int()\n",
    "    labels = labels.squeeze(1).int()\n",
    "\n",
    "    intersection = (preds & labels).float().sum((1, 2)) # zero if mask=0 or Prediction=0\n",
    "    union = (preds | labels).float().sum((1, 2)) # zero if both are 0\n",
    "\n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "\n",
    "    # set Threshold\n",
    "    # thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10\n",
    "\n",
    "    return iou.squeeze(0)\n",
    "\n",
    "\n",
    "def avg_precision(iou_list):\n",
    "    \n",
    "    thresh1 = 0.5 \n",
    "    thresh2 =0.75\n",
    "\n",
    "    # thresh 0.5\n",
    "    iou_list = np.array(iou_list)\n",
    "    iou_list_thresh1= np.where(iou_list > thresh1, 1, 0)\n",
    "    \n",
    "    # thresh 0.75\n",
    "    iou_list_thresh2 = np.where(iou_list > thresh2, 1, 0)\n",
    "    \n",
    "    prec_thresh1 = np.sum(iou_list_thresh1) / len(iou_list_thresh1)\n",
    "    prec_thresh2 = np.sum(iou_list_thresh2) / len(iou_list_thresh2)\n",
    "    \n",
    "    iou_mean = (prec_thresh1 + prec_thresh2) / 2.\n",
    "    \n",
    "    return prec_thresh1, prec_thresh2, iou_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dice_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiceCoef(nn.Module):\n",
    "    \"\"\"Computes Dice Coefficient\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, return_score_per_channel=False):\n",
    "        super(DiceCoef, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.return_score_per_channel = return_score_per_channel\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        per_channel_dice = compute_per_channel_dice(input, target, epsilon=self.epsilon)\n",
    "\n",
    "        if self.return_score_per_channel:\n",
    "            return per_channel_dice\n",
    "        else:\n",
    "            return torch.mean(per_channel_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(optimizer):\n",
    "  return optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "\n",
    "def lr_update(epoch, opt, optimizer):\n",
    "  prev_lr = get_current_lr(optimizer)\n",
    "  if 0 <= epoch < opt.lr_warmup_epoch:\n",
    "    mul_rate = 10 ** (1/opt.lr_warmup_epoch)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= mul_rate\n",
    "    \n",
    "    current_lr = get_current_lr(optimizer)\n",
    "    print(\"LR warm-up : %.7f to %.7f\" % (prev_lr, current_lr))\n",
    "  \n",
    "  else:\n",
    "    if isinstance(opt.lr_decay_epoch, list):\n",
    "      if (epoch+1) in opt.lr_decay_epoch:\n",
    "        for param_group in optimizer.param_groups:\n",
    "          param_group['lr'] = (prev_lr * 0.1)\n",
    "          print(\"LR Decay : %.7f to %.7f\" % (prev_lr, prev_lr * 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataset_trn, optimizer, criterion, epoch, opt,train_writer):\n",
    "    print(\"Start Training...\")\n",
    "    net.train()\n",
    "\n",
    "    losses, total_dices, total_iou = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "    for it, (img, mask) in enumerate(dataset_trn):\n",
    "        # Optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Load Data\n",
    "        img, mask = torch.Tensor(img).float(), torch.Tensor(mask).float()\n",
    "        if opt.use_gpu:\n",
    "            img, mask = img.cuda(non_blocking=True), mask.cuda(non_blocking=True)\n",
    "\n",
    "        # Predict\n",
    "        pred = net(img)\n",
    "\n",
    "        # Loss Calculation\n",
    "        loss = criterion(pred, mask)\n",
    "\n",
    "        pred = pred.sigmoid()\n",
    "        # Backward and step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculation Dice Coef Score\n",
    "        dice = DiceCoef(return_score_per_channel=False)(pred, mask)\n",
    "        total_dices.update(dice.item(), img.size(0))\n",
    "        \n",
    "        # Convert to Binary\n",
    "        zeros = torch.zeros(pred.size())\n",
    "        ones = torch.ones(pred.size())\n",
    "        pred = pred.cpu()\n",
    "\n",
    "        pred = torch.where(pred > 0.5, ones, zeros).cuda() # threshold 0.99\n",
    "\n",
    "        # Calculation IoU Score\n",
    "        iou_score = iou_modified(pred, mask,opt)\n",
    "\n",
    "        total_iou.update(iou_score.mean().item(), img.size(0))\n",
    "\n",
    "        # Stack Results\n",
    "        losses.update(loss.item(), img.size(0))\n",
    "\n",
    "        if (it==0) or (it+1) % 10 == 0:\n",
    "            print('Epoch[%3d/%3d] | Iter[%3d/%3d] | Loss %.4f | Dice %.4f | Iou %.4f'\n",
    "                % (epoch+1, opt.max_epoch, it+1, len(dataset_trn), losses.avg, total_dices.avg, total_iou.avg))\n",
    "\n",
    "    print(\">>> Epoch[%3d/%3d] | Training Loss : %.4f | Dice %.4f | Iou %.4f\\n \"\n",
    "        % (epoch+1, opt.max_epoch, losses.avg, total_dices.avg, total_iou.avg))\n",
    "\n",
    "    train_writer.add_scalar(\"train/loss\", losses.avg, epoch+1)\n",
    "    train_writer.add_scalar(\"train/dice\", total_dices.avg, epoch+1)\n",
    "    train_writer.add_scalar(\"train/IoU\", total_iou.avg, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(dataset_val, net, criterion, epoch, opt, best_iou, best_epoch,train_writer):\n",
    "    print(\"Start Evaluation...\")\n",
    "    net.eval()\n",
    "\n",
    "    # Result containers\n",
    "    losses, total_dices, total_iou = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "    for it, (img, mask) in enumerate(dataset_val):\n",
    "        # Load Data\n",
    "        img, mask = torch.Tensor(img).float(), torch.Tensor(mask).float()\n",
    "        if opt.use_gpu:\n",
    "            img, mask = img.cuda(non_blocking=True), mask.cuda(non_blocking=True)\n",
    "\n",
    "        # Predict\n",
    "        pred = net(img)\n",
    "\n",
    "        # Loss Calculation\n",
    "        loss = criterion(pred, mask)\n",
    "\n",
    "        pred = pred.sigmoid()\n",
    "\n",
    "        # Calculation Dice Coef Score\n",
    "        dice = DiceCoef(return_score_per_channel=False)(pred, mask)\n",
    "        total_dices.update(dice.item(), img.size(0))\n",
    "        \n",
    "        # Convert to Binary\n",
    "        zeros = torch.zeros(pred.size())\n",
    "        ones = torch.ones(pred.size())\n",
    "        pred = pred.cpu()\n",
    "\n",
    "        pred = torch.where(pred > 0.5, ones, zeros).cuda()\n",
    "        \n",
    "        # Calculation IoU Score\n",
    "        iou_score = iou_modified(pred, mask,opt)\n",
    "\n",
    "        total_iou.update(iou_score.mean().item(), img.size(0))\n",
    "\n",
    "        # Stack Results\n",
    "        losses.update(loss.item(), img.size(0))\n",
    "\n",
    "        # if (it==0) or (it+1) % 10 == 0:\n",
    "        #     print('Epoch[%3d/%3d] | Iter[%3d/%3d] | Loss %.4f | Dice %.4f | Iou %.4f'\n",
    "        #         % (epoch+1, opt.max_epoch, it+1, len(dataset_trn), losses.avg, total_dices.avg, total_iou.avg))\n",
    "\n",
    "    print(\">>> Epoch[%3d/%3d] | Test Loss : %.4f | Dice %.4f | Iou %.4f\"\n",
    "        % (epoch+1, opt.max_epoch, losses.avg, total_dices.avg, total_iou.avg))\n",
    "\n",
    "    train_writer.add_scalar(\"valid/loss\", losses.avg, epoch+1)\n",
    "    train_writer.add_scalar(\"valid/dice\", total_dices.avg, epoch+1)\n",
    "    train_writer.add_scalar(\"valid/IoU\", total_iou.avg, epoch+1)\n",
    "\n",
    "    # Update Result\n",
    "    if total_iou.avg > best_iou:\n",
    "        print('Best Score Updated...')\n",
    "        best_iou = total_iou.avg\n",
    "        best_epoch = epoch\n",
    "\n",
    "        # # Remove previous weights pth files\n",
    "        # for path in glob('%s/*.pth' % opt.exp):\n",
    "        #     os.remove(path)\n",
    "\n",
    "        model_filename = '%s/epoch_%04d_iou_%.4f_loss_%.8f.pth' % (opt.exp, epoch+1, best_iou, losses.avg)\n",
    "\n",
    "        # Single GPU\n",
    "        if opt.ngpu == 1:\n",
    "            torch.save(net.state_dict(), model_filename)\n",
    "        # Multi GPU\n",
    "        else:\n",
    "            torch.save(net.module.state_dict(), model_filename)\n",
    "\n",
    "    print('>>> Current best: IoU: %.8f in %3d epoch\\n' % (best_iou, best_epoch+1))\n",
    "    \n",
    "    return best_iou, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset_val, net, opt, save_dir):\n",
    "    print(\"Start Evaluation...\")\n",
    "    net.eval()\n",
    "\n",
    "    iou_scores = []\n",
    "    for idx, (img, mask) in enumerate(dataset_val):\n",
    "        # Load Data\n",
    "        img = torch.Tensor(img).float()\n",
    "        if opt.use_gpu:\n",
    "            img = img.cuda(non_blocking=True)\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            pred = net(img)\n",
    "\t\n",
    "            y = pred.sigmoid()\n",
    "            dice = DiceCoef(return_score_per_channel=False)(y, mask.cuda())\n",
    "            \n",
    "            # Convert to Binary\n",
    "            zeros = torch.zeros(y.size())\n",
    "            ones = torch.ones(y.size())\n",
    "            y = y.cpu()\n",
    "\n",
    "            y = torch.where(y > opt.threshold, ones, zeros) # threshold 0.99\n",
    "            y = Variable(y).cuda()\n",
    "\n",
    "            iou_score = iou_modified(y, mask.cuda(),opt)\n",
    "\n",
    "            if idx%10 ==0:\n",
    "                print(\"{}/{} - dice {:.4f} | IoU {:.4f}\".format(idx+1, len(dataset_val), dice.item(), iou_score.item()))\n",
    "\n",
    "            iou_scores.append(iou_score.item())\n",
    "\n",
    "            if iou_score < 0.75:\n",
    "                ###### Plot & Save Figure #########\n",
    "                origin = img.cpu().numpy()[0,0,:,:] \n",
    "                pred = y.cpu().numpy()[0,0,:,:]\n",
    "                true = mask.cpu().numpy()[0,0,:,:]\t\n",
    "\n",
    "                fig = plt.figure()\n",
    "\n",
    "                ax1 = fig.add_subplot(1,3,1)\n",
    "                ax1.axis(\"off\")\n",
    "                ax1.imshow(origin, cmap = \"gray\")\n",
    "\n",
    "                ax2= fig.add_subplot(1,3,2)\n",
    "                ax2.axis(\"off\")\n",
    "                ax2.imshow(origin,cmap = \"gray\")\n",
    "                ax2.contour(true, cmap='Greens', linewidths=0.3)\n",
    "\n",
    "                ax3 = fig.add_subplot(1,3,3)\n",
    "                ax3.axis(\"off\")\n",
    "                ax3.imshow(origin,cmap = \"gray\")\n",
    "                ax3.contour(pred, cmap='Reds', linewidths=0.3)\n",
    "\n",
    "                plt.axis('off')\n",
    "                plt.subplots_adjust(left = 0, bottom = 0, right = 1, top = 1, hspace = 0, wspace = 0)\n",
    "\n",
    "                plt.savefig(opt.exp + \"/\" + opt.save_dir + \"/original_label_pred_image_file_{}_dice_{:.4f}_iou_{:.4f}.png\".format(idx, dice.item(),iou_score.item()),bbox_inces='tight', dpi=300)\n",
    "                plt.cla()\n",
    "                plt.close(fig)\n",
    "                plt.gray()\n",
    "                ###############################\n",
    "\n",
    "    prec_thresh1, prec_thresh2, iou_mean = avg_precision(iou_scores)\n",
    "\n",
    "    print(\"Presion with threshold 0.5: {}, 0.75: {}, Average: {}\".format(prec_thresh1, prec_thresh2, iou_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "       return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def parse_option(print_option=True):    \n",
    "    p = argparse.ArgumentParser(description='')\n",
    "\n",
    "    # Data Directory\n",
    "    p.add_argument('--data_root', default='../DataSet', type=str, help='root directory of dataset files.')\n",
    "    \n",
    "    # Data augmentation\n",
    "    p.add_argument('--rot_factor', default=30, type=float)\n",
    "    p.add_argument('--scale_factor', default=0.15, type=float)\n",
    "    p.add_argument('--flip', default='True', type=str2bool)\n",
    "    p.add_argument('--trans_factor', default=0.1, type=float)\n",
    "\n",
    "    # Input image\n",
    "    p.add_argument('--crop_size', default=300, type=float, help='Center crop width')\n",
    "    p.add_argument('--input_size', default=224, type=int, help='input resolution using resize process')\n",
    "    p.add_argument('--w_min', default=-100., type=float, help='Min value of HU Windowing')\n",
    "    p.add_argument('--w_max', default=300., type=float, help='Max value of HU Windowing')\n",
    "\n",
    "    # Network\n",
    "    p.add_argument('--base_n_filter', default=32, type=int)\n",
    "\n",
    "    # Optimizer\n",
    "    p.add_argument('--optim', default='Adam', type=str, help='RMSprop | SGD | Adam')\n",
    "    p.add_argument('--lr', default=2e-5, type=float)\n",
    "    p.add_argument('--lr_decay_epoch', default='150', type=str, help=\"decay epochs with comma (ex - '20,40,60')\")\n",
    "    p.add_argument('--lr_warmup_epoch', default=0, type=int)\n",
    "    p.add_argument('--momentum', default=0.99, type=float, help='momentum')\n",
    "    p.add_argument('--wd', default=1e-4, type=float, help='weight decay')\n",
    "    p.add_argument('--no_bias_decay', default='True', type=str2bool, help='weight decay for bias')\n",
    "\n",
    "    # Hyper-parameter\n",
    "    p.add_argument('--batch_size', default=16, type=int, help='use 1 batch size in 3D training.')\n",
    "    p.add_argument('--start_epoch', default=0, type=int)\n",
    "    p.add_argument('--max_epoch', default=300, type=int)\n",
    "    p.add_argument('--threshold', default=0.9, type=float)\n",
    "\n",
    "    # Loss function\n",
    "    p.add_argument('--loss', default='dice', type=str)\n",
    "    p.add_argument('--iou_smooth', default=1e-6, type=float, help='avoid 0/0')\n",
    "\n",
    "    # Resume trained network\n",
    "    p.add_argument('--resume', default='', type=str, help=\"pth file path to resume\")\n",
    "\n",
    "    # Resource option\n",
    "    p.add_argument('--workers', default=10, type=int, help='#data-loading worker-processes')\n",
    "    p.add_argument('--use_gpu', default=\"True\", type=str2bool, help='use gpu or not (cpu only)')\n",
    "    p.add_argument('--gpu_id', default=\"3\", type=str)\n",
    "\n",
    "    # Output directory\n",
    "    p.add_argument('--exp', default='./ckpt_crop_300', type=str, help='checkpoint dir.')\n",
    "    p.add_argument('--save_dir', default='plots', type=str, help='evaluation plot directory')\n",
    "\n",
    "\n",
    "    opt = p.parse_args(\"\")\n",
    "    \n",
    "    # Make output directory\n",
    "    if not os.path.exists(opt.exp):\n",
    "        os.makedirs(opt.exp)\n",
    "\n",
    "    # GPU Setting\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=opt.gpu_id\n",
    "\n",
    "    if opt.use_gpu:\n",
    "        opt.ngpu = len(opt.gpu_id.split(\",\"))\n",
    "    else:\n",
    "        opt.gpu_id = 'cpu'\n",
    "        opt.ngpu = 'cpu'\n",
    "\n",
    "    # lr decay setting\n",
    "    if ',' in opt.lr_decay_epoch:\n",
    "        opt.lr_decay_epoch = opt.lr_decay_epoch.split(',')\n",
    "        opt.lr_decay_epoch = [int(epoch) for epoch in opt.lr_decay_epoch]\n",
    "\n",
    "    if print_option:\n",
    "        print(\"\\n==================================== Options ====================================\\n\")\n",
    "    \n",
    "        print('   Data root : %s' % (opt.data_root))\n",
    "        print()\n",
    "        print('   Data Crop size : Crop to (%d,%d)' % (opt.crop_size,opt.crop_size))\n",
    "        print('   Data input size : Resized to (%d,%d)' % (opt.input_size,opt.input_size))\n",
    "        print()\n",
    "        print('   Base #Filters of Network : %d' % (opt.base_n_filter))\n",
    "        print()\n",
    "        print('   Optimizer : %s (weight decay %f)' % (opt.optim, opt.wd))\n",
    "        print('   Loss function : %s' % opt.loss)\n",
    "        print('   Batch size : %d' % opt.batch_size)\n",
    "        print('   Max epoch : %d' % opt.max_epoch)\n",
    "        print('   Learning rate : %s (linear warm-up until %s / decay at %s)' % (opt.lr, opt.lr_warmup_epoch, opt.lr_decay_epoch))\n",
    "        print()\n",
    "        print('   Resume pre-trained weights path : %s' % opt.resume)\n",
    "        print('   Output dir : %s' % opt.exp)\n",
    "        print()\n",
    "        print('   GPU ID : %s' % opt.gpu_id)\n",
    "        print('   #Workers : %s' % opt.workers)\n",
    "        print('   pytorch version: %s (CUDA : %s)' % (torch.__version__, torch.cuda.is_available()))\n",
    "        print(\"\\n=================================================================================\\n\")\n",
    "\n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option\n",
    "opt = parse_option(print_option=True)\n",
    "\n",
    "# Data Loader\n",
    "dataset_trn, dataset_val = get_dataloader(opt)\n",
    "\n",
    "# Network\n",
    "net = create_model(opt)\n",
    "\n",
    "# Loss Function\n",
    "criterion = get_loss_function(opt)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = get_optimizer(net, opt)\n",
    "\n",
    "# Tensorboard\n",
    "train_writer = SummaryWriter(opt.exp+'/logs/')\n",
    "\n",
    "# Initial Best Score\n",
    "best_iou, best_epoch = [0, 0]\n",
    "\n",
    "for epoch in range(opt.start_epoch, opt.max_epoch):\n",
    "    # Train\n",
    "    train(net, dataset_trn, optimizer, criterion, epoch, opt,train_writer)\n",
    "\n",
    "    # Evaluate\n",
    "    best_iou, best_epoch = validate(dataset_val, net, criterion, epoch, opt, best_iou, best_epoch,train_writer)\n",
    "\n",
    "    lr_update(epoch, opt, optimizer)\n",
    "\n",
    "print('Training done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "opt = parse_option(print_option=False)\n",
    "\n",
    "# Data Loader\n",
    "_, dataset_val = get_dataloader(opt)\n",
    "\n",
    "# Network\n",
    "net = create_model(opt)\n",
    "\n",
    "# Evaluate\n",
    "save_dir = \"./ckpt_beta_100_300\"\n",
    "evaluate(dataset_val, net, opt, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
